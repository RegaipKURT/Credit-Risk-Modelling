{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Credit Risk Modeling - LGD and EAD Models - With Comments - 12-1.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR80ibUZhgXg",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfGXK_8ChgXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYRvvCxChgXm",
        "colab_type": "text"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOZNiNA4hgXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import data.\n",
        "loan_data_preprocessed_backup = pd.read_csv('loan_data_2007_2014_preprocessed.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ1HTUYBhgXt",
        "colab_type": "text"
      },
      "source": [
        "# Explore Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWT9EMkXhgXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loan_data_preprocessed = loan_data_preprocessed_backup.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNk6uwSxhgX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loan_data_preprocessed.columns.values\n",
        "# Displays all column names."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "93GiMJ89hgX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loan_data_preprocessed.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHsxpFSyhgX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loan_data_preprocessed.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI1IoS64hgX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loan_data_defaults = loan_data_preprocessed[loan_data_preprocessed['loan_status'].isin(['Charged Off','Does not meet the credit policy. Status:Charged Off'])]\n",
        "# Here we take only the accounts that were charged-off (written-off)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Q1fjuv-OhgYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loan_data_defaults.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkngjNPzhgYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.options.display.max_rows = None\n",
        "# Sets the pandas dataframe options to display all columns/ rows."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFz2RMo0hgYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loan_data_defaults.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTx9LGjZhgYK",
        "colab_type": "text"
      },
      "source": [
        "# Independent Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "qdK_8b25hgYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loan_data_defaults['mths_since_last_delinq'].fillna(0, inplace = True)\n",
        "# We fill the missing values with zeroes."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBtk4a63hgYO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loan_data_defaults['mths_since_last_delinq'].fillna(loan_data_defaults['mths_since_last_delinq'].max() + 12, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwwk4R8YhgYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loan_data_defaults['mths_since_last_record'].fillna(0, inplace=True)\n",
        "# We fill the missing values with zeroes."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFwdHNnHhgYT",
        "colab_type": "text"
      },
      "source": [
        "# Dependent Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "FlSNpFNdhgYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loan_data_defaults['recovery_rate'] = loan_data_defaults['recoveries'] / loan_data_defaults['funded_amnt']\n",
        "# We calculate the dependent variable for the LGD model: recovery rate.\n",
        "# It is the ratio of recoveries and funded amount."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re1AhPkKhgYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loan_data_defaults['recovery_rate'].describe()\n",
        "# Shows some descriptive statisics for the values of a column."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wBsD3HShgYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loan_data_defaults['recovery_rate'] = np.where(loan_data_defaults['recovery_rate'] > 1, 1, loan_data_defaults['recovery_rate'])\n",
        "loan_data_defaults['recovery_rate'] = np.where(loan_data_defaults['recovery_rate'] < 0, 0, loan_data_defaults['recovery_rate'])\n",
        "# We set recovery rates that are greater than 1 to 1 and recovery rates that are less than 0 to 0."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X49oaKnhgYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loan_data_defaults['recovery_rate'].describe()\n",
        "# Shows some descriptive statisics for the values of a column."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ffN2bJ7hgYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loan_data_defaults['CCF'] = (loan_data_defaults['funded_amnt'] - loan_data_defaults['total_rec_prncp']) / loan_data_defaults['funded_amnt']\n",
        "# We calculate the dependent variable for the EAD model: credit conversion factor.\n",
        "# It is the ratio of the difference of the amount used at the moment of default to the total funded amount."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgRY69sQhgYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loan_data_defaults['CCF'].describe()\n",
        "# Shows some descriptive statisics for the values of a column."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6SFO4QChgYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loan_data_defaults.to_csv('loan_data_defaults.csv')\n",
        "# We save the data to a CSV file."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDFM-z-KhgYm",
        "colab_type": "text"
      },
      "source": [
        "# Explore Dependent Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHLHMfm8hgYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GWl3iabhgYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(loan_data_defaults['recovery_rate'], bins = 100)\n",
        "# We plot a histogram of a variable with 100 bins."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbkrlOtghgYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(loan_data_defaults['recovery_rate'], bins = 50)\n",
        "# We plot a histogram of a variable with 50 bins."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRJ5i844hgYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(loan_data_defaults['CCF'], bins = 100)\n",
        "# We plot a histogram of a variable with 100 bins."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB03QeDihgYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loan_data_defaults['recovery_rate_0_1'] = np.where(loan_data_defaults['recovery_rate'] == 0, 0, 1)\n",
        "# We create a new variable which is 0 if recovery rate is 0 and 1 otherwise."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZOusXkvhgYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loan_data_defaults['recovery_rate_0_1']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV8vzr7UhgY1",
        "colab_type": "text"
      },
      "source": [
        "# LGD Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IskKupr2hgY2",
        "colab_type": "text"
      },
      "source": [
        "### Splitting Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzoLLtQKhgY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R4717Z9hgY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LGD model stage 1 datasets: recovery rate 0 or greater than 0.\n",
        "lgd_inputs_stage_1_train, lgd_inputs_stage_1_test, lgd_targets_stage_1_train, lgd_targets_stage_1_test = train_test_split(loan_data_defaults.drop(['good_bad', 'recovery_rate','recovery_rate_0_1', 'CCF'], axis = 1), loan_data_defaults['recovery_rate_0_1'], test_size = 0.2, random_state = 42)\n",
        "# Takes a set of inputs and a set of targets as arguments. Splits the inputs and the targets into four dataframes:\n",
        "# Inputs - Train, Inputs - Test, Targets - Train, Targets - Test."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFsEELMphgY7",
        "colab_type": "text"
      },
      "source": [
        "### Preparing the Inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mdr3o8EDhgY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_all = ['grade:A',\n",
        "'grade:B',\n",
        "'grade:C',\n",
        "'grade:D',\n",
        "'grade:E',\n",
        "'grade:F',\n",
        "'grade:G',\n",
        "'home_ownership:MORTGAGE',\n",
        "'home_ownership:NONE',\n",
        "'home_ownership:OTHER',\n",
        "'home_ownership:OWN',\n",
        "'home_ownership:RENT',\n",
        "'verification_status:Not Verified',\n",
        "'verification_status:Source Verified',\n",
        "'verification_status:Verified',\n",
        "'purpose:car',\n",
        "'purpose:credit_card',\n",
        "'purpose:debt_consolidation',\n",
        "'purpose:educational',\n",
        "'purpose:home_improvement',\n",
        "'purpose:house',\n",
        "'purpose:major_purchase',\n",
        "'purpose:medical',\n",
        "'purpose:moving',\n",
        "'purpose:other',\n",
        "'purpose:renewable_energy',\n",
        "'purpose:small_business',\n",
        "'purpose:vacation',\n",
        "'purpose:wedding',\n",
        "'initial_list_status:f',\n",
        "'initial_list_status:w',\n",
        "'term_int',\n",
        "'emp_length_int',\n",
        "'mths_since_issue_d',\n",
        "'mths_since_earliest_cr_line',\n",
        "'funded_amnt',\n",
        "'int_rate',\n",
        "'installment',\n",
        "'annual_inc',\n",
        "'dti',\n",
        "'delinq_2yrs',\n",
        "'inq_last_6mths',\n",
        "'mths_since_last_delinq',\n",
        "'mths_since_last_record',\n",
        "'open_acc',\n",
        "'pub_rec',\n",
        "'total_acc',\n",
        "'acc_now_delinq',\n",
        "'total_rev_hi_lim']\n",
        "# List of all independent variables for the models."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhljoiALhgY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_reference_cat = ['grade:G',\n",
        "'home_ownership:RENT',\n",
        "'verification_status:Verified',\n",
        "'purpose:credit_card',\n",
        "'initial_list_status:f']\n",
        "# List of the dummy variable reference categories. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzan0BR3hgZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lgd_inputs_stage_1_train = lgd_inputs_stage_1_train[features_all]\n",
        "# Here we keep only the variables we need for the model."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY9cO8vFhgZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lgd_inputs_stage_1_train = lgd_inputs_stage_1_train.drop(features_reference_cat, axis = 1)\n",
        "# Here we remove the dummy variable reference categories."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvhVYKVehgZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lgd_inputs_stage_1_train.isnull().sum()\n",
        "# Check for missing values. We check whether the value of each row for each column is missing or not,\n",
        "# then sum accross columns."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi8V4ZWghgZJ",
        "colab_type": "text"
      },
      "source": [
        "### Estimating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvHYcS1lhgZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# P values for sklearn logistic regression.\n",
        "\n",
        "# Class to display p-values for logistic regression in sklearn.\n",
        "\n",
        "from sklearn import linear_model\n",
        "import scipy.stats as stat\n",
        "\n",
        "class LogisticRegression_with_p_values:\n",
        "    \n",
        "    def __init__(self,*args,**kwargs):#,**kwargs):\n",
        "        self.model = linear_model.LogisticRegression(*args,**kwargs)#,**args)\n",
        "\n",
        "    def fit(self,X,y):\n",
        "        self.model.fit(X,y)\n",
        "        \n",
        "        #### Get p-values for the fitted model ####\n",
        "        denom = (2.0 * (1.0 + np.cosh(self.model.decision_function(X))))\n",
        "        denom = np.tile(denom,(X.shape[1],1)).T\n",
        "        F_ij = np.dot((X / denom).T,X) ## Fisher Information Matrix\n",
        "        Cramer_Rao = np.linalg.inv(F_ij) ## Inverse Information Matrix\n",
        "        sigma_estimates = np.sqrt(np.diagonal(Cramer_Rao))\n",
        "        z_scores = self.model.coef_[0] / sigma_estimates # z-score for eaach model coefficient\n",
        "        p_values = [stat.norm.sf(abs(x)) * 2 for x in z_scores] ### two tailed test for p-values\n",
        "        \n",
        "        self.coef_ = self.model.coef_\n",
        "        self.intercept_ = self.model.intercept_\n",
        "        #self.z_scores = z_scores\n",
        "        self.p_values = p_values\n",
        "        #self.sigma_estimates = sigma_estimates\n",
        "        #self.F_ij = F_ij"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMGKelIFhgZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reg_lgd_st_1 = LogisticRegression_with_p_values()\n",
        "# We create an instance of an object from the 'LogisticRegression' class.\n",
        "reg_lgd_st_1.fit(lgd_inputs_stage_1_train, lgd_targets_stage_1_train)\n",
        "# Estimates the coefficients of the object from the 'LogisticRegression' class\n",
        "# with inputs (independent variables) contained in the first dataframe\n",
        "# and targets (dependent variables) contained in the second dataframe."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XJ5j-UohgZP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_name = lgd_inputs_stage_1_train.columns.values\n",
        "# Stores the names of the columns of a dataframe in a variable."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geiGQZZdhgZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n",
        "# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\n",
        "summary_table['Coefficients'] = np.transpose(reg_lgd_st_1.coef_)\n",
        "# Creates a new column in the dataframe, called 'Coefficients',\n",
        "# with row values the transposed coefficients from the 'LogisticRegression' object.\n",
        "summary_table.index = summary_table.index + 1\n",
        "# Increases the index of every row of the dataframe with 1.\n",
        "summary_table.loc[0] = ['Intercept', reg_lgd_st_1.intercept_[0]]\n",
        "# Assigns values of the row with index 0 of the dataframe.\n",
        "summary_table = summary_table.sort_index()\n",
        "# Sorts the dataframe by index.\n",
        "p_values = reg_lgd_st_1.p_values\n",
        "# We take the result of the newly added method 'p_values' and store it in a variable 'p_values'.\n",
        "p_values = np.append(np.nan,np.array(p_values))\n",
        "# We add the value 'NaN' in the beginning of the variable with p-values.\n",
        "summary_table['p_values'] = p_values\n",
        "# In the 'summary_table' dataframe, we add a new column, called 'p_values', containing the values from the 'p_values' variable.\n",
        "summary_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACn5q9QthgZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n",
        "summary_table['Coefficients'] = np.transpose(reg_lgd_st_1.coef_)\n",
        "summary_table.index = summary_table.index + 1\n",
        "summary_table.loc[0] = ['Intercept', reg_lgd_st_1.intercept_[0]]\n",
        "summary_table = summary_table.sort_index()\n",
        "p_values = reg_lgd_st_1.p_values\n",
        "p_values = np.append(np.nan,np.array(p_values))\n",
        "summary_table['p_values'] = p_values\n",
        "summary_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5pmxmWRhgZW",
        "colab_type": "text"
      },
      "source": [
        "### Testing the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl1bn8U1hgZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lgd_inputs_stage_1_test = lgd_inputs_stage_1_test[features_all]\n",
        "# Here we keep only the variables we need for the model."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-m8WMt-hgZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lgd_inputs_stage_1_test = lgd_inputs_stage_1_test.drop(features_reference_cat, axis = 1)\n",
        "# Here we remove the dummy variable reference categories."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j8nHAX6hgZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_hat_test_lgd_stage_1 = reg_lgd_st_1.model.predict(lgd_inputs_stage_1_test)\n",
        "# Calculates the predicted values for the dependent variable (targets)\n",
        "# based on the values of the independent variables (inputs) supplied as an argument."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPQkxYDdhgZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_hat_test_lgd_stage_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Htdw_E4thgZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_hat_test_proba_lgd_stage_1 = reg_lgd_st_1.model.predict_proba(lgd_inputs_stage_1_test)\n",
        "# Calculates the predicted probability values for the dependent variable (targets)\n",
        "# based on the values of the independent variables (inputs) supplied as an argument."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol38d2nchgZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_hat_test_proba_lgd_stage_1\n",
        "# This is an array of arrays of predicted class probabilities for all classes.\n",
        "# In this case, the first value of every sub-array is the probability for the observation to belong to the first class, i.e. 0,\n",
        "# and the second value is the probability for the observation to belong to the first class, i.e. 1."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRiN15JXhgZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_hat_test_proba_lgd_stage_1 = y_hat_test_proba_lgd_stage_1[: ][: , 1]\n",
        "# Here we take all the arrays in the array, and from each array, we take all rows, and only the element with index 1,\n",
        "# that is, the second element.\n",
        "# In other words, we take only the probabilities for being 1."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKW95wIuhgZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_hat_test_proba_lgd_stage_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA9yTCPchgZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lgd_targets_stage_1_test_temp = lgd_targets_stage_1_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BD9F0BehgZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lgd_targets_stage_1_test_temp.reset_index(drop = True, inplace = True)\n",
        "# We reset the index of a dataframe."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvppwh3MhgZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_actual_predicted_probs = pd.concat([lgd_targets_stage_1_test_temp, pd.DataFrame(y_hat_test_proba_lgd_stage_1)], axis = 1)\n",
        "# Concatenates two dataframes."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpkaF1adhgZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_actual_predicted_probs.columns = ['lgd_targets_stage_1_test', 'y_hat_test_proba_lgd_stage_1']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1nZXF3PhgZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_actual_predicted_probs.index = lgd_inputs_stage_1_test.index\n",
        "# Makes the index of one dataframe equal to the index of another dataframe."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC-dzHeFhgZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_actual_predicted_probs.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or_7Z8flhgZ5",
        "colab_type": "text"
      },
      "source": [
        "### Estimating the Аccuracy of the Мodel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLfseV0FhgZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tr = 0.5\n",
        "# We create a new column with an indicator,\n",
        "# where every observation that has predicted probability greater than the threshold has a value of 1,\n",
        "# and every observation that has predicted probability lower than the threshold has a value of 0.\n",
        "df_actual_predicted_probs['y_hat_test_lgd_stage_1'] = np.where(df_actual_predicted_probs['y_hat_test_proba_lgd_stage_1'] > tr, 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "HvAlmCRJhgZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.crosstab(df_actual_predicted_probs['lgd_targets_stage_1_test'], df_actual_predicted_probs['y_hat_test_lgd_stage_1'], rownames = ['Actual'], colnames = ['Predicted'])\n",
        "# Creates a cross-table where the actual values are displayed by rows and the predicted values by columns.\n",
        "# This table is known as a Confusion Matrix."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxoC4t8fhgZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.crosstab(df_actual_predicted_probs['lgd_targets_stage_1_test'], df_actual_predicted_probs['y_hat_test_lgd_stage_1'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]\n",
        "# Here we divide each value of the table by the total number of observations,\n",
        "# thus getting percentages, or, rates."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSmuvOUHhgaB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(pd.crosstab(df_actual_predicted_probs['lgd_targets_stage_1_test'], df_actual_predicted_probs['y_hat_test_lgd_stage_1'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[0, 0] + (pd.crosstab(df_actual_predicted_probs['lgd_targets_stage_1_test'], df_actual_predicted_probs['y_hat_test_lgd_stage_1'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[1, 1]\n",
        "# Here we calculate Accuracy of the model, which is the sum of the diagonal rates."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRPnGV_GhgaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2USfzeWdhgaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fpr, tpr, thresholds = roc_curve(df_actual_predicted_probs['lgd_targets_stage_1_test'], df_actual_predicted_probs['y_hat_test_proba_lgd_stage_1'])\n",
        "# Returns the Receiver Operating Characteristic (ROC) Curve from a set of actual values and their predicted probabilities.\n",
        "# As a result, we get three arrays: the false positive rates, the true positive rates, and the thresholds.\n",
        "# we store each of the three arrays in a separate variable."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7oMFckkhgaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(fpr, tpr)\n",
        "# We plot the false positive rate along the x-axis and the true positive rate along the y-axis,\n",
        "# thus plotting the ROC curve.\n",
        "plt.plot(fpr, fpr, linestyle = '--', color = 'k')\n",
        "# We plot a seconary diagonal line, with dashed line style and black color.\n",
        "plt.xlabel('False positive rate')\n",
        "# We name the x-axis \"False positive rate\".\n",
        "plt.ylabel('True positive rate')\n",
        "# We name the x-axis \"True positive rate\".\n",
        "plt.title('ROC curve')\n",
        "# We name the graph \"ROC curve\"."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJBn3k7phgaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AUROC = roc_auc_score(df_actual_predicted_probs['lgd_targets_stage_1_test'], df_actual_predicted_probs['y_hat_test_proba_lgd_stage_1'])\n",
        "# Calculates the Area Under the Receiver Operating Characteristic Curve (AUROC)\n",
        "# from a set of actual values and their predicted probabilities.\n",
        "AUROC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzSgik2-hgaM",
        "colab_type": "text"
      },
      "source": [
        "### Saving the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFHdhExYhgaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z21QSqj8hgaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle.dump(reg_lgd_st_1, open('lgd_model_stage_1.sav', 'wb'))\n",
        "# Here we export our model to a 'SAV' file with file name 'lgd_model_stage_1.sav'."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrmngvdEhgaP",
        "colab_type": "text"
      },
      "source": [
        "### Stage 2 – Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXdNpYFEhgaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lgd_stage_2_data = loan_data_defaults[loan_data_defaults['recovery_rate_0_1'] == 1]\n",
        "# Here we take only rows where the original recovery rate variable is greater than one,\n",
        "# i.e. where the indicator variable we created is equal to 1."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pULRawDMhgaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LGD model stage 2 datasets: how much more than 0 is the recovery rate\n",
        "lgd_inputs_stage_2_train, lgd_inputs_stage_2_test, lgd_targets_stage_2_train, lgd_targets_stage_2_test = train_test_split(lgd_stage_2_data.drop(['good_bad', 'recovery_rate','recovery_rate_0_1', 'CCF'], axis = 1), lgd_stage_2_data['recovery_rate'], test_size = 0.2, random_state = 42)\n",
        "# Takes a set of inputs and a set of targets as arguments. Splits the inputs and the targets into four dataframes:\n",
        "# Inputs - Train, Inputs - Test, Targets - Train, Targets - Test."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojaHR8dehgaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ME9dleMhgaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Since the p-values are obtained through certain statistics, we need the 'stat' module from scipy.stats\n",
        "import scipy.stats as stat\n",
        "\n",
        "# Since we are using an object oriented language such as Python, we can simply define our own \n",
        "# LinearRegression class (the same one from sklearn)\n",
        "# By typing the code below we will ovewrite a part of the class with one that includes p-values\n",
        "# Here's the full source code of the ORIGINAL class: https://github.com/scikit-learn/scikit-learn/blob/7b136e9/sklearn/linear_model/base.py#L362\n",
        "\n",
        "\n",
        "class LinearRegression(linear_model.LinearRegression):\n",
        "    \"\"\"\n",
        "    LinearRegression class after sklearn's, but calculate t-statistics\n",
        "    and p-values for model coefficients (betas).\n",
        "    Additional attributes available after .fit()\n",
        "    are `t` and `p` which are of the shape (y.shape[1], X.shape[1])\n",
        "    which is (n_features, n_coefs)\n",
        "    This class sets the intercept to 0 by default, since usually we include it\n",
        "    in X.\n",
        "    \"\"\"\n",
        "    \n",
        "    # nothing changes in __init__\n",
        "    def __init__(self, fit_intercept=True, normalize=False, copy_X=True,\n",
        "                 n_jobs=1):\n",
        "        self.fit_intercept = fit_intercept\n",
        "        self.normalize = normalize\n",
        "        self.copy_X = copy_X\n",
        "        self.n_jobs = n_jobs\n",
        "\n",
        "    \n",
        "    def fit(self, X, y, n_jobs=1):\n",
        "        self = super(LinearRegression, self).fit(X, y, n_jobs)\n",
        "        \n",
        "        # Calculate SSE (sum of squared errors)\n",
        "        # and SE (standard error)\n",
        "        sse = np.sum((self.predict(X) - y) ** 2, axis=0) / float(X.shape[0] - X.shape[1])\n",
        "        se = np.array([np.sqrt(np.diagonal(sse * np.linalg.inv(np.dot(X.T, X))))])\n",
        "\n",
        "        # compute the t-statistic for each feature\n",
        "        self.t = self.coef_ / se\n",
        "        # find the p-value for each feature\n",
        "        self.p = np.squeeze(2 * (1 - stat.t.cdf(np.abs(self.t), y.shape[0] - X.shape[1])))\n",
        "        return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddGJfIk5hgaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.stats as stat\n",
        "\n",
        "class LinearRegression(linear_model.LinearRegression):\n",
        "    def __init__(self, fit_intercept=True, normalize=False, copy_X=True,\n",
        "                 n_jobs=1):\n",
        "        self.fit_intercept = fit_intercept\n",
        "        self.normalize = normalize\n",
        "        self.copy_X = copy_X\n",
        "        self.n_jobs = n_jobs\n",
        "    def fit(self, X, y, n_jobs=1):\n",
        "        self = super(LinearRegression, self).fit(X, y, n_jobs)\n",
        "        sse = np.sum((self.predict(X) - y) ** 2, axis=0) / float(X.shape[0] - X.shape[1])\n",
        "        se = np.array([np.sqrt(np.diagonal(sse * np.linalg.inv(np.dot(X.T, X))))])\n",
        "        self.t = self.coef_ / se\n",
        "        self.p = np.squeeze(2 * (1 - stat.t.cdf(np.abs(self.t), y.shape[0] - X.shape[1])))\n",
        "        return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwSgB8NHhgaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lgd_inputs_stage_2_train = lgd_inputs_stage_2_train[features_all]\n",
        "# Here we keep only the variables we need for the model."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5THxUJOhgaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lgd_inputs_stage_2_train = lgd_inputs_stage_2_train.drop(features_reference_cat, axis = 1)\n",
        "# Here we remove the dummy variable reference categories."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbnV7gDOhgac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reg_lgd_st_2 = LinearRegression()\n",
        "# We create an instance of an object from the 'LogisticRegression' class.\n",
        "reg_lgd_st_2.fit(lgd_inputs_stage_2_train, lgd_targets_stage_2_train)\n",
        "# Estimates the coefficients of the object from the 'LogisticRegression' class\n",
        "# with inputs (independent variables) contained in the first dataframe\n",
        "# and targets (dependent variables) contained in the second dataframe."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u3grRc3hgad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_name = lgd_inputs_stage_2_train.columns.values\n",
        "# Stores the names of the columns of a dataframe in a variable."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcrMrqIQhgaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n",
        "# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\n",
        "summary_table['Coefficients'] = np.transpose(reg_lgd_st_2.coef_)\n",
        "# Creates a new column in the dataframe, called 'Coefficients',\n",
        "# with row values the transposed coefficients from the 'LogisticRegression' object.\n",
        "summary_table.index = summary_table.index + 1\n",
        "# Increases the index of every row of the dataframe with 1.\n",
        "summary_table.loc[0] = ['Intercept', reg_lgd_st_2.intercept_]\n",
        "# Assigns values of the row with index 0 of the dataframe.\n",
        "summary_table = summary_table.sort_index()\n",
        "# Sorts the dataframe by index.\n",
        "p_values = reg_lgd_st_2.p\n",
        "# We take the result of the newly added method 'p_values' and store it in a variable 'p_values'.\n",
        "p_values = np.append(np.nan,np.array(p_values))\n",
        "# We add the value 'NaN' in the beginning of the variable with p-values.\n",
        "summary_table['p_values'] = p_values.round(3)\n",
        "# In the 'summary_table' dataframe, we add a new column, called 'p_values', containing the values from the 'p_values' variable.\n",
        "summary_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSpNgQ7phgag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n",
        "summary_table['Coefficients'] = np.transpose(reg_lgd_st_2.coef_)\n",
        "summary_table.index = summary_table.index + 1\n",
        "summary_table.loc[0] = ['Intercept', reg_lgd_st_2.intercept_]\n",
        "summary_table = summary_table.sort_index()\n",
        "p_values = reg_lgd_st_2.p\n",
        "p_values = np.append(np.nan,np.array(p_values))\n",
        "summary_table['p_values'] = p_values.round(3)\n",
        "summary_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfuDTk56hgai",
        "colab_type": "text"
      },
      "source": [
        "### Stage 2 – Linear Regression Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK72TpMehgai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lgd_inputs_stage_2_test = lgd_inputs_stage_2_test[features_all]\n",
        "# Here we keep only the variables we need for the model."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH4Aln46hgam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lgd_inputs_stage_2_test = lgd_inputs_stage_2_test.drop(features_reference_cat, axis = 1)\n",
        "# Here we remove the dummy variable reference categories."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQfviwVehgan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lgd_inputs_stage_2_test.columns.values\n",
        "# Calculates the predicted values for the dependent variable (targets)\n",
        "# based on the values of the independent variables (inputs) supplied as an argument."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9rumBQmhgap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_hat_test_lgd_stage_2 = reg_lgd_st_2.predict(lgd_inputs_stage_2_test)\n",
        "# Calculates the predicted values for the dependent variable (targets)\n",
        "# based on the values of the independent variables (inputs) supplied as an argument."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB3zxMXDhgaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lgd_targets_stage_2_test_temp = lgd_targets_stage_2_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNMo9H9_hgas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lgd_targets_stage_2_test_temp = lgd_targets_stage_2_test_temp.reset_index(drop = True)\n",
        "# We reset the index of a dataframe."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4kta4OVhgau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.concat([lgd_targets_stage_2_test_temp, pd.DataFrame(y_hat_test_lgd_stage_2)], axis = 1).corr()\n",
        "# We calculate the correlation between actual and predicted values."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "K5gKpl44hgax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(lgd_targets_stage_2_test - y_hat_test_lgd_stage_2)\n",
        "# We plot the distribution of the residuals."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZC0y0Prhgay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle.dump(reg_lgd_st_2, open('lgd_model_stage_2.sav', 'wb'))\n",
        "# Here we export our model to a 'SAV' file with file name 'lgd_model_stage_1.sav'."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoCv6MSbhga0",
        "colab_type": "text"
      },
      "source": [
        "### Combining Stage 1 and Stage 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or92aFIThga0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_hat_test_lgd_stage_2_all = reg_lgd_st_2.predict(lgd_inputs_stage_1_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEHVKF4chga2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_hat_test_lgd_stage_2_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3XLZI1rhga3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_hat_test_lgd = y_hat_test_lgd_stage_1 * y_hat_test_lgd_stage_2_all\n",
        "# Here we combine the predictions of the models from the two stages."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lbm4tthhga4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame(y_hat_test_lgd).describe()\n",
        "# Shows some descriptive statisics for the values of a column."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViCjt6Tbhga5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_hat_test_lgd = np.where(y_hat_test_lgd < 0, 0, y_hat_test_lgd)\n",
        "y_hat_test_lgd = np.where(y_hat_test_lgd > 1, 1, y_hat_test_lgd)\n",
        "# We set predicted values that are greater than 1 to 1 and predicted values that are less than 0 to 0."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t35Ru3RBhga7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame(y_hat_test_lgd).describe()\n",
        "# Shows some descriptive statisics for the values of a column."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n79Xtuxhga8",
        "colab_type": "text"
      },
      "source": [
        "# EAD Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjY_jJClhga9",
        "colab_type": "text"
      },
      "source": [
        "### Estimation and Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWuLFW7dhga-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EAD model datasets\n",
        "ead_inputs_train, ead_inputs_test, ead_targets_train, ead_targets_test = train_test_split(loan_data_defaults.drop(['good_bad', 'recovery_rate','recovery_rate_0_1', 'CCF'], axis = 1), loan_data_defaults['CCF'], test_size = 0.2, random_state = 42)\n",
        "# Takes a set of inputs and a set of targets as arguments. Splits the inputs and the targets into four dataframes:\n",
        "# Inputs - Train, Inputs - Test, Targets - Train, Targets - Test."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne10bgtBhga_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ead_inputs_train.columns.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KiWKo4yhgbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ead_inputs_train = ead_inputs_train[features_all]\n",
        "# Here we keep only the variables we need for the model."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wqjusEthgbB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ead_inputs_train = ead_inputs_train.drop(features_reference_cat, axis = 1)\n",
        "# Here we remove the dummy variable reference categories."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxaVIjHwhgbD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reg_ead = LinearRegression()\n",
        "# We create an instance of an object from the 'LogisticRegression' class.\n",
        "reg_ead.fit(ead_inputs_train, ead_targets_train)\n",
        "# Estimates the coefficients of the object from the 'LogisticRegression' class\n",
        "# with inputs (independent variables) contained in the first dataframe\n",
        "# and targets (dependent variables) contained in the second dataframe."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw3OXlQwhgbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_name = ead_inputs_train.columns.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SjH9TB9OhgbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n",
        "# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\n",
        "summary_table['Coefficients'] = np.transpose(reg_ead.coef_)\n",
        "# Creates a new column in the dataframe, called 'Coefficients',\n",
        "# with row values the transposed coefficients from the 'LogisticRegression' object.\n",
        "summary_table.index = summary_table.index + 1\n",
        "# Increases the index of every row of the dataframe with 1.\n",
        "summary_table.loc[0] = ['Intercept', reg_ead.intercept_]\n",
        "# Assigns values of the row with index 0 of the dataframe.\n",
        "summary_table = summary_table.sort_index()\n",
        "# Sorts the dataframe by index.\n",
        "p_values = reg_ead.p\n",
        "# We take the result of the newly added method 'p_values' and store it in a variable 'p_values'.\n",
        "p_values = np.append(np.nan,np.array(p_values))\n",
        "# We add the value 'NaN' in the beginning of the variable with p-values.\n",
        "summary_table['p_values'] = p_values\n",
        "# In the 'summary_table' dataframe, we add a new column, called 'p_values', containing the values from the 'p_values' variable.\n",
        "summary_table"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}